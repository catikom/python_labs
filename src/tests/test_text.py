import pytest
import sys

sys.path.append("C:/Users/user/Desktop/python_labs/")
from src.lib.text import normalize, tokenize, count_freq, top_n


@pytest.mark.parametrize(
    "source, expected",
    [
        ("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("—ë–∂–∏–∫, –Å–ª–∫a", "–µ–∂–∏–∫, –µ–ª–∫a"),
        ("Hello\r\nWorld", "hello world"),
        ("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"),
        ("", ""),
    ],
)
def test_normalize_basic(source, expected):
    assert normalize(source) == expected


@pytest.mark.parametrize(
    "source, expected",
    [
        ("–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä!", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]),
        ("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]),
        ("2025 –≥–æ–¥", ["2025", "–≥–æ–¥"]),
        ("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]),
        ("    –º–Ω–æ–æ–æ–æ–æ–æ–≥–æ –Ω–µ–Ω—É–∂–Ω–æ–≥–æ!!", ["–º–Ω–æ–æ–æ–æ–æ–æ–≥–æ", "–Ω–µ–Ω—É–∂–Ω–æ–≥–æ"]),
        ("", []),
    ],
)
def test_tokenize_basic(source, expected):
    assert tokenize(source) == expected


@pytest.mark.parametrize(
    "tokens, expected",
    [
        (["a", "b", "a", "c", "b", "a"], {"a": 3, "b": 2, "c": 1}),
        ([], {}),
        (["test", "test", "test"], {"test": 3}),
        (["üåç", "üöÄ", "üåç"], {"üåç": 2, "üöÄ": 1}),
    ],
)
def test_count_freq_and_top_n(tokens, expected):
    assert count_freq(tokens) == expected


@pytest.mark.parametrize(
    "words, n, expected",
    [
        ({"b": 5, "a": 5, "c": 3, "d": 2}, 2, [("a", 5), ("b", 5)]),
        ({"x": 10}, 5, [("x", 10)]),
        ({}, 3, []),
        ({"a": 1, "b": 1}, 0, []),
    ],
)
def test_top_n_tie_breaker(words, n, expected):
    assert top_n(words, n) == expected
